# Ollama Configuration
# Make sure Ollama is installed and running locally
# Default: http://localhost:11434
# Install Ollama: https://ollama.ai
# Pull llama2 model: ollama pull llama2
OLLAMA_HOST=http://localhost:11434
